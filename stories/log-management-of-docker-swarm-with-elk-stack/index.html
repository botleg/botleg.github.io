<!DOCTYPE html> <html> <head> <title>Log Management for Docker Swarm with ELK Stack - botleg</title> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="In the previous article, we covered how to monitor a Docker Swarm. As a follow up, in this article, we will go through setting up a dynamic solution for log ..."> <meta name="author" content="Hanzel Jesheen"> <meta name="twitter:card" content="Setup a dynamic log management solution for Docker Swarm with Elasticsearch, Logstash, Kibana and Logspout." /> <meta name="twitter:site" content="@HanzelJesheen" /> <meta name="twitter:creator" content="@HanzelJesheen" /> <meta property="og:url" content="https://botleg.com/stories/log-management-of-docker-swarm-with-elk-stack/" /> <meta property="og:type" content="article" /> <meta property="og:title" content="Log Management for Docker Swarm with ELK Stack" /> <meta property="og:description" content="Setup a dynamic log management solution for Docker Swarm with Elasticsearch, Logstash, Kibana and Logspout." /> <meta property="og:image" content="" /> <meta property="og:site_name" content="botleg" /> <meta property="article:author" content="Hanzel Jesheen" /> <meta itemprop="name" content="Log Management for Docker Swarm with ELK Stack"> <meta itemprop="description" content="In the previous article, we covered how to monitor a Docker Swarm. As a follow up, in this article, we will go through setting up a dynamic solution for log ..."> <link rel="canonical" href="https://botleg.com/stories/log-management-of-docker-swarm-with-elk-stack/"> <link rel="alternate" type="application/rss+xml" title="botleg" href="https://botleg.com/feed" /> </head> <body> <div class="page-content"> <div class="post"> <header style="background:#136a8a;background-image:linear-gradient(160deg, #267871, #136a8a);background-image:-moz-linear-gradient(160deg, #267871, #136a8a);background-image:-webkit-linear-gradient(160deg, #267871, #136a8a);background-image:-o-linear-gradient(160deg, #267871, #136a8a);background-image:-ms-linear-gradient(160deg, #267871, #136a8a);"> <section class="site-header"> <div class="wrapper"> <a class="site-title" href="/">Botleg</a> <div id="search"> <a href="/feed/index.xml"><span class="rss-icon"> <svg viewBox="0 0 24 24" width="30px" height="30px"> <use xlink:href="/assets/images/sprites.svg#rss"></use> </svg></span> </a> <form id="searchform"> <input type="text" placeholder="search" id="searchbox"> </form> </div> </div> </section> <section class="post-header"> <div class="header-content"> <h1 class="post-title">Log Management for Docker Swarm with ELK Stack</h1> <a class="icon gh" href="https://github.com/botleg/swarm-logging" target="blank"><span>View Code</span></a> </div> </section> </header> <div class="wrapper"> <section> <article class="post-content"> <p class="post-meta">Written by Hanzel Jesheen on Apr 30, 2017 | <a href="/categories/devops">devops</a> </p> <p>In the <a href="/stories/monitoring-docker-swarm-with-cadvisor-influxdb-and-grafana/">previous article</a>, we covered how to monitor a Docker Swarm. As a follow up, in this article, we will go through setting up a dynamic solution for log management in a docker swarm. We will be collecting logs from all the containers in all the nodes of the swarm. With all this data, we would also be able to do querying and analysis.</p> <p>We will use the very popular <a href="https://www.elastic.co/products">ELK stack</a> from <a href="https://www.elastic.co">elastic</a> for log management. This includes <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>, <a href="https://www.elastic.co/products/logstash">Logstash</a> and <a href="https://www.elastic.co/products/kibana">Kibana</a>. <code class="highlighter-rouge">Elasticsearch</code> is the database to store the log data and query for it. <code class="highlighter-rouge">Logstash</code> is a log collection pipeline that ingests logs from multiple sources and feeds it to Elasticsearch. <code class="highlighter-rouge">Kibana</code> is the web UI to display Elasticsearch data. We will also deploy <a href="https://github.com/gliderlabs/logspout">logspout</a> to all nodes in the swarm, which will connect to docker daemon in the host to collect all logs and feed it to logstash. All these tools are open-source and can be deployed as a container.</p> <p>We will use the Docker Swarm Mode to build the cluster and deploy these services as a stack. This allows for us to collect logs from any container without making any changes. Any logs that can be seen with <code class="highlighter-rouge">docker logs</code> command will be automatically collected. Also, logs will collected from any new nodes joining the swarm. All the files used for this project can be found <a href="https://github.com/botleg/swarm-logging">here</a>.</p> <h2 id="the-stack">The Stack</h2> <p>The tools that we use for log management stack are all open-source. These tools are all container-friendly and easily scalable. Our stack comprises of the following tools.</p> <h1 id="elasticsearch">Elasticsearch</h1> <blockquote> <p>Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases.</p> </blockquote> <p><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> is essentially a database that store JSON documents, based on <a href="https://lucene.apache.org/">Apache Lucene</a>. It is a very powerful and efficent full-text search engine with a REST API. In this article, we will deploy only a single instance of Elasticsearch. However, for any sort of production setup, you would need to setup a cluster of Elasticsearch. To setup an Elasticsearch cluster, check <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#docker-cli-run-prod-mode">here</a> and <a href="https://github.com/deviantony/docker-elk/wiki/Elasticsearch-cluster">here</a>.</p> <h1 id="logstash">Logstash</h1> <blockquote> <p>Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash”.</p> </blockquote> <p><a href="https://www.elastic.co/products/logstash">Logstash</a> ingests data from many source, do some processing and then feed it to Elasticsearch. In this demo, we are pushing the logs to a single instance for logstash. We can have multiple instances of logstash and even have a layer of <a href="https://www.elastic.co/products/beats">Beats shippers</a> that push logs to logstash.</p> <h1 id="kibana">Kibana</h1> <blockquote> <p>Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack.</p> </blockquote> <p><a href="https://www.elastic.co/products/kibana">Kibana</a> is a web UI for elastic stack. It lets you see and query the log data. You can create graphs and dashboards with the data. Kibana stores all the information including the graphs in elasticsearch. So, Kibana in itself is stateless and can be scaled independently.</p> <h1 id="logspout">Logspout</h1> <blockquote> <p>Log routing for Docker container logs</p> </blockquote> <p><a href="https://github.com/gliderlabs/logspout">Logspout</a> from Gliderlabs is log router that connects to the docker daemon and collects logs from all its container. Here, we use the logstash plugin to push it to logstash. We use the <a href="https://github.com/Bekt/logspout-logstash">bekt/logspout-logstash</a> image for this. We deploy it to all nodes in the cluster to get all the logs.</p> <h2 id="docker-swarm-mode">Docker Swarm Mode</h2> <p>The first thing to do is to create a <a href="https://docs.docker.com/engine/swarm/">Docker Swarm</a> with the <a href="https://docs.docker.com/machine/">docker-machine</a>. We will be creating a swarm with one manager named <code class="highlighter-rouge">manager</code> and two workers named <code class="highlighter-rouge">agent1</code> and <code class="highlighter-rouge">agent2</code>. To follow along with the demonstration, you need to have the following prerequisites:</p> <ul> <li><code class="highlighter-rouge">Docker</code>: version &gt;= 1.13, to support Docker Compose File version 3 and Swarm Mode.</li> <li><code class="highlighter-rouge">Docker Machine</code>: version &gt;= 0.8</li> <li><code class="highlighter-rouge">Docker Compose</code>: version &gt;= 1.10, to support Docker Compose file version 3</li> </ul> <p>I have explained how to do this in my previous article, <a href="/stories/monitoring-docker-swarm-with-cadvisor-influxdb-and-grafana/">Monitoring Docker Swarm with cAdvisor, InfluxDB and Grafana</a>. Follow the steps in the section <code class="highlighter-rouge">Docker Swarm Mode</code> of that article to create and setup the Swarm. Once the swarm is setup, you can see the hosts with <code class="highlighter-rouge">docker node ls</code> command. The output of this command must look something like this.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS
3j231njh03spl0j8h67z069cy <span class="k">*</span>  manager   Ready   Active        Leader
muxpteij6aldkixnl31f0asar    agent1    Ready   Active
y2gstaqpqix1exz09nyjn8z41    agent2    Ready   Active</code></pre></figure> <h2 id="docker-stack">Docker Stack</h2> <p>For elasticsearch to not give <code class="highlighter-rouge">Out of Memory</code> errors, we need set <code class="highlighter-rouge">vm.max_map_count</code> of the kernel of VMs to atleast <code class="highlighter-rouge">262144</code>. To do this, run the following commands.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker-machine ssh manager sudo sysctl -w vm.max_map_count<span class="o">=</span>262144
docker-machine ssh agent1 sudo sysctl -w vm.max_map_count<span class="o">=</span>262144
docker-machine ssh agent2 sudo sysctl -w vm.max_map_count<span class="o">=</span>262144</code></pre></figure> <p>We will define the entire stack for logging in the <code class="highlighter-rouge">docker-stack.yml</code> file. This will contain the information about different services including the deploy stratergies. This file will be in the <a href="https://docs.docker.com/compose/compose-file/">docker-compose v3</a> format. We can then deploy it with one command. The <code class="highlighter-rouge">docker-stack.yml</code> file is given below.</p> <figure class="highlight"><pre><code class="language-conf" data-lang="conf"><span class="n">version</span>: <span class="s1">'3'</span>

<span class="n">services</span>:
  <span class="n">elasticsearch</span>:
    <span class="n">image</span>: <span class="n">docker</span>.<span class="n">elastic</span>.<span class="n">co</span>/<span class="n">elasticsearch</span>/<span class="n">elasticsearch</span>:<span class="m">5</span>.<span class="m">3</span>.<span class="m">2</span>
    <span class="n">environment</span>:
      <span class="n">ES_JAVA_OPTS</span>: <span class="s1">'-Xms256m -Xmx256m'</span>
      <span class="n">xpack</span>.<span class="n">security</span>.<span class="n">enabled</span>: <span class="s1">'false'</span>
      <span class="n">xpack</span>.<span class="n">monitoring</span>.<span class="n">enabled</span>: <span class="s1">'false'</span>
      <span class="n">xpack</span>.<span class="n">graph</span>.<span class="n">enabled</span>: <span class="s1">'false'</span>
      <span class="n">xpack</span>.<span class="n">watcher</span>.<span class="n">enabled</span>: <span class="s1">'false'</span>
    <span class="n">volumes</span>:
      - <span class="n">esdata</span>:/<span class="n">usr</span>/<span class="n">share</span>/<span class="n">elasticsearch</span>/<span class="n">data</span>
    <span class="n">deploy</span>:
      <span class="n">replicas</span>: <span class="m">1</span>
      <span class="n">placement</span>:
        <span class="n">constraints</span>:
          - <span class="n">node</span>.<span class="n">hostname</span> == <span class="n">agent1</span>

  <span class="n">logstash</span>:
    <span class="n">image</span>: <span class="n">docker</span>.<span class="n">elastic</span>.<span class="n">co</span>/<span class="n">logstash</span>/<span class="n">logstash</span>:<span class="m">5</span>.<span class="m">3</span>.<span class="m">2</span>
    <span class="n">volumes</span>:
      - ./<span class="n">logstash</span>/<span class="n">logstash</span>.<span class="n">conf</span>:/<span class="n">usr</span>/<span class="n">share</span>/<span class="n">logstash</span>/<span class="n">pipeline</span>/<span class="n">logstash</span>.<span class="n">conf</span>
    <span class="n">depends_on</span>:
      - <span class="n">elasticsearch</span>
    <span class="n">deploy</span>:
      <span class="n">replicas</span>: <span class="m">1</span>

  <span class="n">logspout</span>:
    <span class="n">image</span>: <span class="n">bekt</span>/<span class="n">logspout</span>-<span class="n">logstash</span>
    <span class="n">environment</span>:
      <span class="n">ROUTE_URIS</span>: <span class="s1">'logstash://logstash:5000'</span>
    <span class="n">volumes</span>:
      - /<span class="n">var</span>/<span class="n">run</span>/<span class="n">docker</span>.<span class="n">sock</span>:/<span class="n">var</span>/<span class="n">run</span>/<span class="n">docker</span>.<span class="n">sock</span>
    <span class="n">depends_on</span>:
      - <span class="n">logstash</span>
    <span class="n">deploy</span>:
      <span class="n">mode</span>: <span class="n">global</span>
      <span class="n">restart_policy</span>:
        <span class="n">condition</span>: <span class="n">on</span>-<span class="n">failure</span>
        <span class="n">delay</span>: <span class="m">30</span><span class="n">s</span>

  <span class="n">kibana</span>:
    <span class="n">image</span>: <span class="n">docker</span>.<span class="n">elastic</span>.<span class="n">co</span>/<span class="n">kibana</span>/<span class="n">kibana</span>:<span class="m">5</span>.<span class="m">3</span>.<span class="m">2</span>
    <span class="n">ports</span>:
      - <span class="s1">'80:5601'</span>
    <span class="n">depends_on</span>:
      - <span class="n">elasticsearch</span>
    <span class="n">environment</span>:
      <span class="n">ELASTICSEARCH_URL</span>: <span class="s1">'http://elasticsearch:9200'</span>
      <span class="n">XPACK_SECURITY_ENABLED</span>: <span class="s1">'false'</span>
      <span class="n">XPACK_MONITORING_ENABLED</span>: <span class="s1">'false'</span>
    <span class="n">deploy</span>:
      <span class="n">replicas</span>: <span class="m">1</span>

<span class="n">volumes</span>:
  <span class="n">esdata</span>:
    <span class="n">driver</span>: <span class="n">local</span></code></pre></figure> <p>We will start by saying that we are using the version 3 of <code class="highlighter-rouge">docker-compose</code> file. We have following 4 services in the stack.</p> <h1 id="elasticsearch-1">elasticsearch</h1> <p>For the Elasticsearch service, we use the offical <code class="highlighter-rouge">docker.elastic.co/elasticsearch/elasticsearch</code> image. With the <code class="highlighter-rouge">ES_JAVA_OPTS</code> environment variable, we will set the heap space, 256MB as the minimum and maximum here. Also, the official image comes with <a href="https://www.elastic.co/products/x-pack">X-Pack</a> installed, which will take care of security, monitoring, alerting etc. of the elastic stack. For this demo, we will disable the X-Pack. We set <code class="highlighter-rouge">xpack.security.enabled</code>, <code class="highlighter-rouge">xpack.monitoring.enabled</code>, <code class="highlighter-rouge">xpack.graph.enabled</code> and <code class="highlighter-rouge">xpack.watcher.enabled</code> as <code class="highlighter-rouge">false</code> to do this. We will setup a docker volume named <code class="highlighter-rouge">esdata</code> mounted at <code class="highlighter-rouge">/usr/share/elasticsearch/data</code> to store all Elasticsearch data. In the deploy key, we are saying that we need one copy of it. We are placing this service in <code class="highlighter-rouge">agent1</code> node. This is done to make sure that the volume is always in that host.</p> <h1 id="logstash-1">logstash</h1> <p>We will use the official <code class="highlighter-rouge">docker.elastic.co/logstash/logstash</code> image for logstash. We also need to setup a custom configuration file for logstash to define its pipeline. The configuration file <code class="highlighter-rouge">logstash.conf</code> is given below.</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="n">input</span> <span class="p">{</span>
  <span class="n">udp</span> <span class="p">{</span>
    <span class="n">port</span>  <span class="o">=&gt;</span> <span class="mi">5000</span>
    <span class="n">codec</span> <span class="o">=&gt;</span> <span class="n">json</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">filter</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">[</span><span class="n">docker</span><span class="p">][</span><span class="n">image</span><span class="p">]</span> <span class="o">=~</span> <span class="sr">/logstash/</span> <span class="p">{</span>
    <span class="n">drop</span> <span class="p">{</span> <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">output</span> <span class="p">{</span>
  <span class="n">elasticsearch</span> <span class="p">{</span> <span class="n">hosts</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="s2">"elasticsearch:9200"</span><span class="p">]</span> <span class="p">}</span>
  <span class="n">stdout</span> <span class="p">{</span> <span class="n">codec</span> <span class="o">=&gt;</span> <span class="n">rubydebug</span> <span class="p">}</span>
<span class="p">}</span></code></pre></figure> <p>The config file contains three sections. The <code class="highlighter-rouge">input</code> section defines where logstash is listening for log data. In this case, logstash will listen at port <code class="highlighter-rouge">5000</code> where log will be coming in <code class="highlighter-rouge">json</code> format and using <code class="highlighter-rouge">UDP</code> protocol. The <code class="highlighter-rouge">filter</code> section can do some processing with the log data. Here, we will drop all the logs coming from <code class="highlighter-rouge">logstash</code> image, as those are duplicates. The <code class="highlighter-rouge">output</code> section defines where logstash is feeding the data to. Here, we will send it to <code class="highlighter-rouge">elasticsearch</code> service at port <code class="highlighter-rouge">9200</code>. We will use the <code class="highlighter-rouge">rubydebug</code> codec as it will use <a href="https://github.com/awesome-print/awesome_print">Ruby Awesome Print</a> library to pretty-print the data.</p> <p>Put this file, <code class="highlighter-rouge">logstash.conf</code> in a folder named <code class="highlighter-rouge">logstash</code>. We are creating a volume to share this file at the location <code class="highlighter-rouge">/usr/share/logstash/pipeline</code> of the container. This will then be read automatically when logstash starts up. Also, this service depends on Elasticsearch as it feed data to it. As before, we only need one copy of this service.</p> <h1 id="logspout-1">logspout</h1> <p>For <code class="highlighter-rouge">logspout</code>, we use <a href="https://hub.docker.com/r/bekt/logspout-logstash">bekt/logspout-logstash</a> image. This image contains <a href="https://github.com/gliderlabs/logspout">logspout</a> with <a href="https://github.com/looplab/logspout-logstash">logstash plugin</a>. We need to provide the <code class="highlighter-rouge">ROUTE_URIS</code> environment variable with the location of logstash. In this case, it is <code class="highlighter-rouge">logstash:5000</code> with <code class="highlighter-rouge">logstash</code> protocol. We also need to create a volume for the Docker socket, <code class="highlighter-rouge">/var/run/docker.sock</code>. This lets the container to attach to the docker daemon in the host and collect all the logs. This service depends on Logstash as it feed data to it. We need to deploy this service to all the nodes. So we use the <code class="highlighter-rouge">global</code> mode for <code class="highlighter-rouge">deploy</code>. Also, there is a chance for this service to fail bacause logstash is not ready. So, we setup a restart policy that restarts the service if it fails with a delay of 30 seconds.</p> <h1 id="kibana-1">kibana</h1> <p>We use the official <code class="highlighter-rouge">docker.elastic.co/kibana/kibana</code> image and expose the port <code class="highlighter-rouge">5601</code> of the container to port <code class="highlighter-rouge">80</code> of the host. The <code class="highlighter-rouge">router mesh</code> feature will then let us access kibana from port 80 of any host in the swarm. This service get data from Elasticsearch, so depended on it. We need to set the <code class="highlighter-rouge">ELASTICSEARCH_URL</code> to the elasticsearch service, <code class="highlighter-rouge">http://elasticsearch:9200</code> in this case. We also set the <code class="highlighter-rouge">XPACK_SECURITY_ENABLED</code> and <code class="highlighter-rouge">XPACK_MONITORING_ENABLED</code> environment variables as <code class="highlighter-rouge">false</code>, to disable <code class="highlighter-rouge">X-Pack</code>. Here also, we only need one instance of this service.</p> <p>Finally, at the end of the file, we have the <code class="highlighter-rouge">volumes</code> key with the <code class="highlighter-rouge">esdata</code> volumes. We are using the <code class="highlighter-rouge">local</code> driver for this so the data will be stored in the host containing the service. As Elasticsearch service is placed in <code class="highlighter-rouge">agent1</code>, the volume is always present there.</p> <p>To deploy this stack, save the above file as <code class="highlighter-rouge">docker-stack.yml</code> and run the following command.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker stack deploy -c docker-stack.yml elk</code></pre></figure> <p>This will start the services in the stack named <code class="highlighter-rouge">elk</code>. The first time takes more time as the nodes have to download the images. To see the services in the stack, you can use the command <code class="highlighter-rouge">docker stack services elk</code>, the output of the command will look like this.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">ID            NAME                   MODE        REPLICAS  IMAGE
07h9zishcka5  logging_logspout       global      3/3       bekt/logspout-logstash:latest
7upb3emlhsft  logging_kibana         replicated  1/1       docker.elastic.co/kibana/kibana:5.3.2
puxx0x4txa50  logging_logstash       replicated  1/1       docker.elastic.co/logstash/logstash:5.3.2
wyjkad4do7oi  logging_elasticsearch  replicated  1/1       docker.elastic.co/elasticsearch/elasticsearch:5.3.2</code></pre></figure> <p>You can see the running containers in the stack with the command <code class="highlighter-rouge">docker stack ps elk</code>. Its output will look like this.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">ID            NAME                                        IMAGE                                                NODE     DESIRED STATE  CURRENT STATE               ERROR                      PORTS
jqr1m6ts21p3  logging_logspout.pt27y28y0t5zzph3oi72tmy58  bekt/logspout-logstash:latest                        agent2   Running        Running about a minute ago
4zwvtt3momu3  logging_logspout.9m6jopba7lr0o40hw9nwe7zfb  bekt/logspout-logstash:latest                        agent1   Running        Running about a minute ago
mgpsi68gcvd9  logging_logspout.ub1sl7d5fy9dbnlx8um67a03t  bekt/logspout-logstash:latest                        manager  Running        Running about a minute ago
unz9qrfit8li  logging_logstash.1                          logstash:alpine                                      agent1   Running        Running 2 minutes ago
jjin64lsw2dr  logging_kibana.1                            docker.elastic.co/kibana/kibana:5.3.0                agent2   Running        Running 2 minutes ago
orzfd05rzq8e  logging_elasticsearch.1                     docker.elastic.co/elasticsearch/elasticsearch:5.3.0  agent1   Running        Running 3 minutes ago</code></pre></figure> <h2 id="setup-kibana">Setup Kibana</h2> <p>Once the services have started, you can open kibana with the following command:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">open http://<span class="sb">`</span>docker-machine ip manager<span class="sb">`</span></code></pre></figure> <p><img src="/assets/images/kibana-index@2x.jpg" srcset="/assets/images/kibana-index@1x.jpg 300w, /assets/images/kibana-index@2x.jpg 600w, /assets/images/kibana-index@3x.jpg 900w" sizes="(min-width: 960px) 900px, 100vw" alt="Add default index pattern" class="center-image" /> <em class="image-caption">Add Default Index Pattern</em></p> <p>It might take some time for kibana to load up. So, if the page doesn’t load up, wait for a few minutes and try again. The first time kibana is opened, it will ask to specify a default index pattern. Logstash will create index starting with <code class="highlighter-rouge">logstash-</code>, so the default index pattern is <code class="highlighter-rouge">logstash-*</code>. Also the Time-field name is <code class="highlighter-rouge">@timestamp</code>. This is the field that stores the time when the log entry is made. Click on the <code class="highlighter-rouge">Create</code> button to set it up. Now go to the <code class="highlighter-rouge">Discover</code> tab to see all the log entries. To the left, you can see all the fields indentified from the logs. If you click on any field, you can see the top values and its percentages. The <code class="highlighter-rouge">docker.image</code> field will give the docker image used, the <code class="highlighter-rouge">docker.name</code> field gives the container name, etc.</p> <p>To test this setup, we will deploy another stack and see its logs from here. For this we will use a simple stack containing a single service. It will be <a href="https://www.nginx.com">nginx</a> webserver serving a static webpage. The image used for this is <code class="highlighter-rouge">hanzel/nginx-html</code> and we expose port <code class="highlighter-rouge">80</code> of the container as port <code class="highlighter-rouge">8000</code> in the host. Also, we deploy 5 instances of this service, just to see logs from different instances. The stack file for this stack <code class="highlighter-rouge">nginx.yml</code> is given below.</p> <figure class="highlight"><pre><code class="language-conf" data-lang="conf"><span class="n">version</span>: <span class="s1">'3'</span>

<span class="n">services</span>:
  <span class="n">nginx</span>:
    <span class="n">image</span>: <span class="n">hanzel</span>/<span class="n">nginx</span>-<span class="n">html</span>
    <span class="n">ports</span>:
      - <span class="s2">"8000:80"</span>
    <span class="n">deploy</span>:
      <span class="n">replicas</span>: <span class="m">5</span></code></pre></figure> <p>We will deploy this stack using the command,</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker stack deploy -c nginx.yml nginx</code></pre></figure> <p>You can see the services in the <code class="highlighter-rouge">nginx</code> stack with the command <code class="highlighter-rouge">docker stack services nginx</code>. The output of the command will look something like this.</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">ID            NAME         MODE        REPLICAS  IMAGE
dl8k8w2wief5  nginx_nginx  replicated  5/5       hanzel/nginx-html:latest</code></pre></figure> <p>You can open up the nginx webserver with the following command</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">open http://<span class="sb">`</span>docker-machine ip manager:8000<span class="sb">`</span></code></pre></figure> <p>Refresh the page a few times to generate some log entries. Now you can see the logs in Kibana. Use the query <code class="highlighter-rouge">docker.image: nginx AND (200 OR 304)</code> in Kibana. This filter will fetch the log entries where the docker image name contains <code class="highlighter-rouge">nginx</code> and the log message contains <code class="highlighter-rouge">200</code> or <code class="highlighter-rouge">304</code>. If you look at the <code class="highlighter-rouge">docker.image</code> field on the left, you can see that we are reading logs from multiple containers across different hosts.</p> <p><img src="/assets/images/kibana-nginx@2x.jpg" srcset="/assets/images/kibana-nginx@1x.jpg 300w, /assets/images/kibana-nginx@2x.jpg 600w, /assets/images/kibana-nginx@3x.jpg 900w" sizes="(min-width: 960px) 900px, 100vw" alt="Kibana logs for nginx" class="center-image" /> <em class="image-caption">Kibana logs for nginx</em></p> <h2 id="conclusion">Conclusion</h2> <p>In this article, we have deployed a dynamic log management solution for our docker swarm. Once the stack is setup, logs are automatically collected from all the containers across all the hosts in the swarm. To do this, we have used popular open-source tools like Elasticsearch, Logstash, Kibana and Logspout.</p> <p>Once you are done with the demonstration, you can remove the stack with command,</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker stack rm elk
docker stack rm nginx</code></pre></figure> <p>If you are done with the VMs created for the demo, you can stop and remove then with the following commands,</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker-machine stop manager agent1 agent2
docker-machine rm -f manager agent1 agent2</code></pre></figure> <p class="post-meta">Tags: docker, swarm, log, logging, elk, elasticsearch, logstash, kibana, logspout, elastic, xpack, docker-machine, swarmmode, and devops </p> </article> <article class="post-comments" id="comments-box"> <h1 class="page-title">Comments</h1> <form id="new-comment"> <input type="text" id="nameField" placeholder="Enter your name" required><br/> <textarea id="textField" placeholder="Your comments please." required></textarea><br/> <button type="submit" id="commentBtn">Post Comment</button> </form> <div id="comments"></div> </article> </section> <aside class="post-aside"> <div class ="aside" id="cat-list"> <h1 class="page-heading">Categories</h1> <a href="/categories/misc">Misc</a><br/> <a href="/categories/jekyll">Jekyll</a><br/> <a href="/categories/cloud">Cloud</a><br/> <a href="/categories/Node.js">Node.js</a><br/> <a href="/categories/javascript">Javascript</a><br/> <a href="/categories/devops">Devops</a><br/> </div> <div class ="aside"> <h1 class="page-heading">Related Stories</h1> <a href=/stories/monitoring-docker-swarm-with-cadvisor-influxdb-and-grafana/>Monitoring Docker Swarm with cAdvisor, InfluxDB and Grafana</a><br/><a href=/stories/auto-scaling-with-docker/>Auto Scaling with Docker</a><br/><a href=/stories/blue-green-deployment-with-docker/>Building Blue-Green Deployment with Docker</a><br/><a href=/stories/load-balancing-with-docker-swarm/>Load Balancing with Docker Swarm</a><br/><a href=/stories/setup-gitlab-for-docker-based-development/>Setup Gitlab for Docker based development</a><br/> </div> <p class="rss-subscribe">subscribe <a href="/feed">via RSS</a></p> <a href="#">Scroll to Top</a> </aside> </div> </div> </div> <footer class="site-footer"> <div class="footer-wrap"> <h2 class="footer-heading"><a href="/">botleg</a></h2> <div class="footer-col-wrapper"> <div class="footer-col footer-col-1"> <ul class="contact-list"> <li>By Hanzel Jesheen</li> <li><a href="mailto:blog@botleg.com">blog@botleg.com</a></li> </ul> </div> <div class="footer-col footer-col-2"> <ul class="social-media-list"> <li> <a href="https://github.com/botleg"> <span class="footer-icon"> <svg viewBox="0 0 16 16" width="16px" height="16px"> <use xlink:href="/assets/images/sprites.svg#gh"></use> </svg> </span> <span class="username">github</span> </a> </li> <li class="username"> <span class="footer-icon"> <svg viewBox="0 0 24 24" width="16px" height="16px"> <use xlink:href="/assets/images/sprites.svg#rss"></use> </svg> </span> subscribe <a href="/feed">via RSS</a> </li> </ul> </div> <div class="footer-col footer-col-3"> <p>Hi, botleg is a blog where you can find tutorials and techniques in web development, devops, databases, cloud computing and anything related to technology.</p> </div> </div> </div> </footer> <style type="text/css">body,h1,h2{font-weight:300}.wrapper:after{clear:both}html{box-sizing:border-box}body{background-color:#fdfdfd;color:#111;font-family:Helvetica,Arial,sans-serif;font-size:18px;line-height:1.6;}.wrapper section{margin-top:30px}a{color:#2a7ae2;text-decoration:none}.site-header{min-height:60px;position:relative}.site-title{margin-bottom:0;float:left;font-size:28px;line-height:60px;text-transform:lowercase;color:#e8e8e8}.post-header{box-align:center;justify-content:center;color:#fdfdfd;display:flex;height:300px;margin-bottom:10px}.rss-icon,#searchbox{display:none;}.post-header .header-content{text-align:center;max-width:600px}.icon{background-position:10px center;background-repeat:no-repeat;background-size:30px;padding:0 30px 0 45px}.icon span{color:#fdfdfd;display:none}@media screen and (min-width:400px){.icon span{display:inline}}.post-title{color:#e8e8e8;font-size:36px}.page-content,.post-content{margin-bottom:20px}.page-content h2,.post-content h2{font-size:32px}</style> <script> var cb = function() { var l = document.createElement('link'); l.rel = 'stylesheet'; l.href = '/assets/main.css?v310'; var h = document.getElementsByTagName('head')[0]; h.parentNode.insertBefore(l, h); }; var raf = requestAnimationFrame || mozRequestAnimationFrame || webkitRequestAnimationFrame || msRequestAnimationFrame; if (raf) raf(cb); else window.addEventListener('load', cb); </script> <script async type="text/javascript" src="/assets/js/post.js"></script> </body> </html>
